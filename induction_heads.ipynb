{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n",
      "Intel MKL WARNING: Support of Intel(R) Streaming SIMD Extensions 4.2 (Intel(R) SSE4.2) enabled only processors has been deprecated. Intel oneAPI Math Kernel Library 2025.0 will require Intel(R) Advanced Vector Extensions (Intel(R) AVX) instructions.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/ashishneupane/opt/anaconda3/envs/transformers/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import einops\n",
    "from typing import List, Optional, Tuple\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import webbrowser\n",
    "import gdown\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly_utils\n",
    "\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "import circuitsvis as cv\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device=device(type='cpu')\n"
     ]
    }
   ],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "torch.set_grad_enabled(False)\n",
    "#device_name = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device_name = \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(f\"{device=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    d_model=768,\n",
    "    d_head=64,\n",
    "    n_heads=12,\n",
    "    n_layers=2,\n",
    "    n_ctx=2048,\n",
    "    d_vocab=50278,\n",
    "    attention_dir=\"causal\",\n",
    "    attn_only=True, # defaults to False\n",
    "    tokenizer_name=\"EleutherAI/gpt-neox-20b\", \n",
    "    seed=398,\n",
    "    use_attn_result=True,\n",
    "    normalization_type=None, # defaults to \"LN\", i.e. layernorm with weights & biases\n",
    "    positional_embedding_type=\"shortformer\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'exists'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m weights_dir \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mattn_only_2L_half.pth\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m weights_dir\u001b[39m.\u001b[39;49mexists():\n\u001b[1;32m      3\u001b[0m     url \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mhttps://drive.google.com/uc?id=1vcZLJnJoYKQs-2KOjkd6LvHZrkSdoxhu\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m     output \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(weights_dir)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'exists'"
     ]
    }
   ],
   "source": [
    "weights_dir = \"attn_only_2L_half.pth\"\n",
    "if not os.path.abspath(weights_dir).exists():\n",
    "    url = \"https://drive.google.com/uc?id=1vcZLJnJoYKQs-2KOjkd6LvHZrkSdoxhu\"\n",
    "    output = str(weights_dir)\n",
    "    gdown.download(url, output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = HookedTransformer(cfg)\n",
    "pretrained_weights = torch.load(weights_dir, map_location=device)\n",
    "model.load_state_dict(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"We think that powerful HookedTransformer, significantly superhuman machine intelligence is more likely than not to be created this century by HookedTransformer. If current HookedTransformer machine learning techniques were scaled up to this level with HookedTransformer, we think they would by default produce HookedTransformer systems that are deceptive or manipulative HookedTransformer, and that no solid plans are known for HookedTransformer on how to avoid this. HookedTransformer.\"\n",
    "#input_text = \"Try inputting different text, and see how stable your results are. Do you always get the same classifications for heads\"\n",
    "#input_text = \"Again, you are strongly recommended to read the corresponding section of the glossary, before continuing (or this LessWrong post). In brief, however, the induction circuit consists of a previous token head in layer 0 and an induction head in layer 1, where the induction head learns to attend to the token immediately after copies of the current token via K-Composition with the previous token\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits, cache = model.run_with_cache(input_text, remove_batch_dim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_n = 0\n",
    "attention_pattern_0 = cache[\"pattern\", layer_n]\n",
    "tokens = model.to_str_tokens(input_text)\n",
    "print(f\"Layer {layer_n} Head Attention Patterns:\")\n",
    "display(cv.attention.attention_heads(\n",
    "    tokens=tokens,\n",
    "    attention=attention_pattern_0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_n = 1\n",
    "attention_pattern_0 = cache[\"pattern\", layer_n]\n",
    "tokens = model.to_str_tokens(input_text)\n",
    "print(f\"Layer {layer_n} Head Attention Patterns:\")\n",
    "display(cv.attention.attention_heads(\n",
    "    tokens=tokens,\n",
    "    attention=attention_pattern_0\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def current_attn_detector(cache: ActivationCache, batch_n = 0, threshold=3) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be current-token heads\n",
    "    '''\n",
    "    current_attn_heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        attention_pattern = cache[\"pattern\", layer]\n",
    "        if cache.has_batch_dim:\n",
    "            if batch_n is not None:\n",
    "                attention_pattern = attention_pattern[batch_n].squeeze()\n",
    "            else:\n",
    "                # batch_n = None === average activations across batch.\n",
    "                attention_pattern = attention_pattern.mean(dim=0)\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            curr_attention_pattern = attention_pattern[head].squeeze()\n",
    "            # Get the diagonal values (self-attention values)\n",
    "            diag = torch.diagonal(curr_attention_pattern)\n",
    "            \n",
    "            norm_attention_pattern = (curr_attention_pattern - curr_attention_pattern.mean()) / curr_attention_pattern.std()\n",
    "            \n",
    "            norm_diag = (diag - curr_attention_pattern.mean()) / curr_attention_pattern.std()\n",
    "            # Check if the mean diagonal value is significantly larger than the mean off-diagonal value\n",
    "            if norm_diag.mean() > norm_attention_pattern.mean() + threshold * norm_attention_pattern.std():\n",
    "                current_attn_heads.append(f\"{layer}.{head}\")\n",
    "    return current_attn_heads\n",
    "\n",
    "def prev_attn_detector(cache: ActivationCache, batch_n = 0, threshold=3) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be prev-token heads\n",
    "    '''\n",
    "    current_attn_heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        attention_pattern = cache[\"pattern\", layer]\n",
    "        if cache.has_batch_dim:\n",
    "            if batch_n is not None:\n",
    "                attention_pattern = attention_pattern[batch_n].squeeze()\n",
    "            else:\n",
    "                # batch_n = None === average activations across batch.\n",
    "                attention_pattern = attention_pattern.mean(dim=0)\n",
    "\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            curr_attention_pattern = attention_pattern[head].squeeze()\n",
    "            # Get the first subdiagonal values (prev-token-attention values)\n",
    "            diag = torch.diag(curr_attention_pattern, diagonal=-1)\n",
    "            \n",
    "            norm_attention_pattern = (curr_attention_pattern - curr_attention_pattern.mean()) / curr_attention_pattern.std()\n",
    "            \n",
    "            norm_diag = (diag - curr_attention_pattern.mean()) / curr_attention_pattern.std()\n",
    "            # Check if the mean first token attn value is significantly larger than the mean off-diagonal value\n",
    "            if norm_diag.mean() > norm_attention_pattern.mean() + threshold * norm_attention_pattern.std():\n",
    "                current_attn_heads.append(f\"{layer}.{head}\")\n",
    "    return current_attn_heads\n",
    "\n",
    "\n",
    "def first_attn_detector(cache: ActivationCache, batch_n = 0, threshold=3) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be first-token heads\n",
    "    '''\n",
    "    current_attn_heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        attention_pattern = cache[\"pattern\", layer]\n",
    "        if cache.has_batch_dim:\n",
    "            if batch_n is not None:\n",
    "                attention_pattern = attention_pattern[batch_n].squeeze()\n",
    "            else:\n",
    "                # batch_n = None === average activations across batch.\n",
    "                attention_pattern = attention_pattern.mean(dim=0)\n",
    "\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            curr_attention_pattern = attention_pattern[head].squeeze()\n",
    "            # Get the first column (first token attention values)\n",
    "            first_token_attn = curr_attention_pattern[:, 0]\n",
    "            \n",
    "            norm_attention_pattern = (curr_attention_pattern - curr_attention_pattern.mean()) / curr_attention_pattern.std()\n",
    "            \n",
    "            norm_diag = (first_token_attn - curr_attention_pattern.mean()) / curr_attention_pattern.std()\n",
    "            # Check if the mean diagonal value is significantly larger than the mean off-diagonal value\n",
    "            if norm_diag.mean() > norm_attention_pattern.mean() + threshold * norm_attention_pattern.std():\n",
    "                current_attn_heads.append(f\"{layer}.{head}\")\n",
    "    return current_attn_heads\n",
    "\n",
    "def create_induction_mask(tokens):\n",
    "    \"\"\"\n",
    "    Create a mask where a value 1 at position (i, j) where i > j and \n",
    "    the value at index i is exactly equal to the value at index j.\n",
    "\n",
    "    Args:\n",
    "    tokens: A 1D tensor of tokens.\n",
    "\n",
    "    Returns:\n",
    "    A 2D mask tensor.\n",
    "    \"\"\"\n",
    "    # Get the length of the sequence\n",
    "    seq_len = tokens.size()[0]\n",
    "\n",
    "    # Initialize an empty mask\n",
    "    mask = torch.zeros((seq_len, seq_len), dtype=torch.bool)\n",
    "\n",
    "    # Compare each token to all previous tokens - 1, for induction, skip the first token\n",
    "    for i in range(1, seq_len):\n",
    "        mask[i, 1:i] = (tokens[i] == tokens[:i-1])\n",
    "\n",
    "    return mask\n",
    "\n",
    "def induction_head_detector(cache: ActivationCache, tokens, batch_n = 0, threshold=3) -> List[str]:\n",
    "    '''\n",
    "    Returns a list e.g. [\"0.2\", \"1.4\", \"1.9\"] of \"layer.head\" which you judge to be prev-token heads\n",
    "    '''\n",
    "    # except for the BOS token at index 0, \n",
    "    # for the second half of the sequence, index repeated_seq_len + i is exactly equal to index i\n",
    "    # Induction heads will have high activation at position (repeated_seq_len + 1, i + 1) as it would be\n",
    "    # attending to the next token of the last time this token was seen!\n",
    "    # now, alternatively, we can get the activations of the last time this token was seen, without asserting\n",
    "    # equality, maybe that can work for the previous non-repeated examples too?\n",
    "\n",
    "    induction_mask = create_induction_mask(tokens.squeeze())\n",
    "    current_attn_heads = []\n",
    "    for layer in range(model.cfg.n_layers):\n",
    "        attention_pattern = cache[\"pattern\", layer]\n",
    "        if cache.has_batch_dim:\n",
    "            if batch_n is not None:\n",
    "                attention_pattern = attention_pattern[batch_n].squeeze()\n",
    "            else:\n",
    "                # batch_n = None === average activations across batch.\n",
    "                attention_pattern = attention_pattern.mean(dim=0)\n",
    "\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            curr_attention_pattern = attention_pattern[head].squeeze()\n",
    "            induction_activations = curr_attention_pattern[induction_mask]\n",
    "            \n",
    "            norm_attention_pattern = (curr_attention_pattern - curr_attention_pattern.mean()) / curr_attention_pattern.std()\n",
    "            norm_induction = (induction_activations - curr_attention_pattern.mean()) / curr_attention_pattern.std()\n",
    "            # Check if the mean induction token attn value is significantly larger than the mean activations\n",
    "            if norm_induction.mean() > norm_attention_pattern.mean() + threshold * norm_attention_pattern.std():\n",
    "                current_attn_heads.append(f\"{layer}.{head}\")\n",
    "    return current_attn_heads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Heads attending to current token  = \", \", \".join(current_attn_detector(cache, threshold=3)))\n",
    "print(\"Heads attending to previous token = \", \", \".join(prev_attn_detector(cache, threshold=3)))\n",
    "print(\"Heads attending to first token    = \", \", \".join(first_attn_detector(cache, threshold=4)))\n",
    "print(\"Induction heads: \", \", \".join(induction_head_detector(cache, model.to_tokens(input_text), threshold=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_repeated_tokens(\n",
    "    model: HookedTransformer, seq_len: int, batch: int = 1\n",
    ") -> torch.Tensor:\n",
    "    '''\n",
    "    Generates a sequence of repeated random tokens\n",
    "\n",
    "    Int[torch.Tensor, \"batch full_seq_len\"]\n",
    "    Outputs are:\n",
    "        rep_tokens: [batch, 1+2*seq_len]\n",
    "    '''\n",
    "    prefix = (torch.ones(batch, 1) * model.tokenizer.bos_token_id).long()\n",
    "    first_half = torch.randint(low = 0, high = model.cfg.d_vocab, size = torch.Size([batch, seq_len]), dtype=torch.long)\n",
    "    repeated_tokens = torch.concat([prefix, first_half, first_half], dim=1)\n",
    "    return repeated_tokens\n",
    "\n",
    "def run_and_cache_model_repeated_tokens(model: HookedTransformer, seq_len: int, batch: int = 1) -> Tuple[torch.Tensor, torch.Tensor, ActivationCache]:\n",
    "    '''\n",
    "    Generates a sequence of repeated random tokens, and runs the model on it, returning logits, tokens and cache\n",
    "\n",
    "    Should use the `generate_repeated_tokens` function above\n",
    "\n",
    "    Outputs are:\n",
    "        rep_tokens: [batch, 1+2*seq_len]\n",
    "        rep_logits: [batch, 1+2*seq_len, d_vocab]\n",
    "        rep_cache: The cache of the model run on rep_tokens\n",
    "    '''\n",
    "    rep_tokens = generate_repeated_tokens(model, seq_len, batch=batch)\n",
    "    rep_logits, rep_cache = model.run_with_cache(rep_tokens)\n",
    "    print(rep_tokens.size(), rep_logits.size(), type(rep_cache))\n",
    "    return rep_tokens, rep_logits, rep_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(logits, tokens):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    print(logits.size(), tokens.size())\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)    \n",
    "    log_probs = log_probs[:,:-1].gather(-1, index=tokens[:, 1:].unsqueeze(-1)).squeeze(-1)\n",
    "    print(log_probs.size())\n",
    "    return log_probs\n",
    "\n",
    "def plot_losses(log_probs, seq_len):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    # Calculate the mean log probabilities for the first and second halves of the sequence\n",
    "    first_half = log_probs[:seq_len]\n",
    "    second_half = log_probs[seq_len:]\n",
    "\n",
    "    # Plot the difference\n",
    "    plt.plot(first_half, label=\"first half\")\n",
    "    plt.plot(second_half, label=\"second_half\")\n",
    "    plt.ylabel('Log Probability of correct prediction')\n",
    "    plt.xlabel('Token position')\n",
    "    plt.legend()\n",
    "    # plt.title(f'Loss Difference for \"{rep_str}\"')\n",
    "    plt.grid()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 20\n",
    "batch = 1\n",
    "(rep_tokens, rep_logits, rep_cache) = run_and_cache_model_repeated_tokens(model, seq_len, batch)\n",
    "rep_str = model.to_str_tokens(rep_tokens)\n",
    "rep_cache.remove_batch_dim()\n",
    "model.reset_hooks()\n",
    "log_probs = get_log_probs(rep_logits, rep_tokens).squeeze()\n",
    "\n",
    "print(f\"Performance on the first half: {log_probs[:seq_len].mean():.3f}\")\n",
    "print(f\"Performance on the second half: {log_probs[seq_len:].mean():.3f}\")\n",
    "plotly_utils.plot_loss_difference(log_probs, rep_str, seq_len)\n",
    "plot_losses(log_probs, seq_len)\n",
    "print(rep_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_n = 0\n",
    "print(f\"Layer {layer_n} attention heads\")\n",
    "display(cv.attention.attention_heads(\n",
    "    tokens=rep_str,\n",
    "    attention=rep_cache[\"pattern\", layer_n]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_n = 1\n",
    "print(f\"Layer {layer_n} attention heads\")\n",
    "display(cv.attention.attention_heads(\n",
    "    tokens=rep_str,\n",
    "    attention=rep_cache[\"pattern\", layer_n]\n",
    "))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Heads attending to current token  = \", \", \".join(current_attn_detector(rep_cache, threshold=3)))\n",
    "print(\"Heads attending to previous token = \", \", \".join(prev_attn_detector(rep_cache, threshold=3)))\n",
    "print(\"Heads attending to first token    = \", \", \".join(first_attn_detector(rep_cache, threshold=3)))\n",
    "print(\"Induction heads: \", \", \".join(induction_head_detector(rep_cache, rep_tokens, threshold=2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Heads attending to current token  =  0.1, 0.9, 0.11, 1.6, 1.7\n",
    "Heads attending to previous token =  0.0, 0.4, 0.7, 0.9\n",
    "Heads attending to first token    =  0.2, 0.3, 0.6, 1.0, 1.1, 1.2, 1.3, 1.4, 1.6, 1.8, 1.9, 1.10, 1.11\n",
    "Induction heads:  1.4, 1.10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 1000 examples and find common heads!\n",
    "batch = 1000\n",
    "seq_len = 51\n",
    "\n",
    "prefix = (torch.ones(batch, 1) * model.tokenizer.bos_token_id).long()\n",
    "random_tokens = torch.randint(low = 0, high = model.cfg.d_vocab, size = torch.Size([batch, seq_len-1]), dtype=torch.long)\n",
    "batch_tokens = torch.concat([prefix, random_tokens, random_tokens[:, 15:20], random_tokens[:, 5:10]], dim=1)\n",
    "batch_logits, batch_cache = model.run_with_cache(batch_tokens)\n",
    "print(batch_tokens.size(), batch_logits.size(), type(batch_cache))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "curr_token_heads = []\n",
    "prev_token_heads = []\n",
    "first_token_heads = []\n",
    "induction_heads = []\n",
    "for batch_n in range(batch):\n",
    "    curr_token_heads.append(current_attn_detector(batch_cache, batch_n=batch_n, threshold=3))\n",
    "    prev_token_heads.append(prev_attn_detector(batch_cache, batch_n=batch_n, threshold=3))\n",
    "    first_token_heads.append(first_attn_detector(batch_cache, batch_n=batch_n, threshold=3))\n",
    "    induction_heads.append(induction_head_detector(batch_cache, batch_tokens[batch_n].squeeze(), batch_n=batch_n, threshold=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools\n",
    "import collections\n",
    "import pandas as pd\n",
    "\n",
    "def plot_histogram(data, title):\n",
    "    \"\"\"\n",
    "    Plot a histogram of counts across a 2D array of string values.\n",
    "\n",
    "    Args:\n",
    "    data: A 2D list of strings.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    # Flatten the 2D list into a 1D list\n",
    "    flattened_data = list(itertools.chain.from_iterable(data))\n",
    "\n",
    "    # Count the occurrence of each string\n",
    "    counter = collections.Counter(flattened_data)\n",
    "\n",
    "    # Convert the counter to a DataFrame for plotting\n",
    "    df = pd.DataFrame.from_dict(counter, orient='index').reset_index()\n",
    "    df = df.rename(columns={'index':'Head', 0:'Count'})\n",
    "\n",
    "    # Create the plot\n",
    "    fig = px.histogram(df, x='Head', y='Count', title=title)\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_histogram(curr_token_heads, \"Current Token Attention Heads\")\n",
    "plot_histogram(prev_token_heads, \"Previous Token Attention Heads\")\n",
    "plot_histogram(first_token_heads, \"First Token Attention Heads\")\n",
    "plot_histogram(induction_heads, \"Induction Heads\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention_heads(attention_scores, layer_n):\n",
    "    \"\"\"\n",
    "    Plot the attention scores of all heads.\n",
    "\n",
    "    Args:\n",
    "    attention_scores: A tensor of shape [n_heads, seq_len, seq_len]\n",
    "    representing the attention scores for each head.\n",
    "    \"\"\"\n",
    "    n_heads, seq_len, _ = attention_scores.shape\n",
    "\n",
    "    # Create a subplot with 4 rows and 3 columns (for 12 heads)\n",
    "    fig = sp.make_subplots(rows=4, cols=3, subplot_titles=[f'Head {i}' for i in range(n_heads)])\n",
    "\n",
    "    for i in range(n_heads):\n",
    "        # Compute the row and column indices for the subplot\n",
    "        row = i // 3 + 1\n",
    "        col = i % 3 + 1\n",
    "\n",
    "        # Plot the attention scores for this head\n",
    "        img = px.imshow(attention_scores[i], color_continuous_scale='viridis', binary_string=True)\n",
    "\n",
    "        fig.add_trace(\n",
    "            img.data[0],\n",
    "            row=row,\n",
    "            col=col\n",
    "        )\n",
    "\n",
    "    fig.update_layout(height=800, width=800, title_text=f\"Attention Scores for Each Head at layer {layer_n}\")\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_n = 0\n",
    "plot_attention_heads(batch_cache[\"pattern\", layer_n].mean(dim=0), layer_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_n = 1\n",
    "plot_attention_heads(batch_cache[\"pattern\", layer_n].mean(dim=0), layer_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix_idx = (torch.ones(1, 1) * model.tokenizer.bos_token_id).long()\n",
    "random_token_idx = torch.randint(low = 1, high = model.cfg.d_vocab, size = torch.Size([1, seq_len-1]), dtype=torch.long)\n",
    "batch_token_idx = torch.concat([prefix_idx, random_token_idx, random_token_idx[:, 15:20], random_token_idx[:, 5:10]], dim=1)\n",
    "print(\"Heads attending to current token  = \", \", \".join(current_attn_detector(batch_cache, batch_n = None, threshold=3)))\n",
    "print(\"Heads attending to previous token = \", \", \".join(prev_attn_detector(batch_cache, batch_n = None, threshold=3)))\n",
    "print(\"Heads attending to first token    = \", \", \".join(first_attn_detector(batch_cache, batch_n = None, threshold=4)))\n",
    "print(\"Induction heads: \", \", \".join(induction_head_detector(batch_cache, batch_token_idx, batch_n = None, threshold=2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
