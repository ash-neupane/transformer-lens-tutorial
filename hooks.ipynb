{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import einops\n",
    "from typing import List, Optional, Tuple\n",
    "import functools\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display\n",
    "import webbrowser\n",
    "import gdown\n",
    "from matplotlib import pyplot as plt\n",
    "import plotly_utils\n",
    "from transformer_lens.hook_points import HookPoint\n",
    "from transformer_lens import utils, HookedTransformer, HookedTransformerConfig, FactoredMatrix, ActivationCache\n",
    "import circuitsvis as cv\n",
    "import functools\n",
    "MAIN = __name__ == \"__main__\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "torch.set_grad_enabled(False)\n",
    "#device_name = \"mps\" if torch.backends.mps.is_available() else \"cpu\"\n",
    "device_name = \"cpu\"\n",
    "device = torch.device(device_name)\n",
    "print(f\"{device=}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfg = HookedTransformerConfig(\n",
    "    d_model=768,\n",
    "    d_head=64,\n",
    "    n_heads=12,\n",
    "    n_layers=2,\n",
    "    n_ctx=2048,\n",
    "    d_vocab=50278,\n",
    "    attention_dir=\"causal\",\n",
    "    attn_only=True, # defaults to False\n",
    "    tokenizer_name=\"EleutherAI/gpt-neox-20b\", \n",
    "    seed=398,\n",
    "    use_attn_result=True,\n",
    "    normalization_type=None, # defaults to \"LN\", i.e. layernorm with weights & biases\n",
    "    positional_embedding_type=\"shortformer\"\n",
    ")\n",
    "weights_dir = \"attn_only_2L_half.pth\"\n",
    "if not Path(weights_dir).exists():\n",
    "    url = \"https://drive.google.com/uc?id=1vcZLJnJoYKQs-2KOjkd6LvHZrkSdoxhu\"\n",
    "    output = str(weights_dir)\n",
    "    gdown.download(url, output)\n",
    "model = HookedTransformer(cfg)\n",
    "pretrained_weights = torch.load(weights_dir, map_location=device)\n",
    "model.load_state_dict(pretrained_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We make a tensor to store the induction score for each head.\n",
    "# We put it on the model's device to avoid needing to move things between the GPU and CPU, which can be slow.\n",
    "global induction_score_store, determinant_store, rank_store\n",
    "induction_score_store = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
    "rank_store = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_repeated_tokens(\n",
    "    model: HookedTransformer, seq_len: int, batch: int = 1\n",
    ") -> torch.Tensor:\n",
    "    '''\n",
    "    Generates a sequence of repeated random tokens\n",
    "\n",
    "    Int[torch.Tensor, \"batch full_seq_len\"]\n",
    "    Outputs are:\n",
    "        rep_tokens: [batch, 1+2*seq_len]\n",
    "    '''\n",
    "    prefix = (torch.ones(batch, 1) * model.tokenizer.bos_token_id).long()\n",
    "    first_half = torch.randint(low = 0, high = model.cfg.d_vocab, size = torch.Size([batch, seq_len]), dtype=torch.long)\n",
    "    repeated_tokens = torch.concat([prefix, first_half, first_half], dim=1)\n",
    "    return repeated_tokens\n",
    "\n",
    "def run_and_cache_model_repeated_tokens(model: HookedTransformer, seq_len: int, batch: int = 1) -> Tuple[torch.Tensor, torch.Tensor, ActivationCache]:\n",
    "    '''\n",
    "    Generates a sequence of repeated random tokens, and runs the model on it, returning logits, tokens and cache\n",
    "\n",
    "    Should use the `generate_repeated_tokens` function above\n",
    "\n",
    "    Outputs are:\n",
    "        rep_tokens: [batch, 1+2*seq_len]\n",
    "        rep_logits: [batch, 1+2*seq_len, d_vocab]\n",
    "        rep_cache: The cache of the model run on rep_tokens\n",
    "    '''\n",
    "    rep_tokens = generate_repeated_tokens(model, seq_len, batch=batch)\n",
    "    rep_logits, rep_cache = model.run_with_cache(rep_tokens)\n",
    "    print(rep_tokens.size(), rep_logits.size(), type(rep_cache))\n",
    "    return rep_tokens, rep_logits, rep_cache\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_induction_mask_rep_tokens(seq_len):\n",
    "    \"\"\"\n",
    "    Create a mask where a value 1 at position (i, j) where i > j and \n",
    "    the value at index i is exactly equal to the value at index j.\n",
    "    Assumes the tokens are generated with generate_repeated_tokens, i.e.\n",
    "    position 0 is the BOS token, 1, ..., seq_len is repeated twice.\n",
    "\n",
    "    Args:\n",
    "    tokens: A 1D tensor of tokens.\n",
    "\n",
    "    Returns:\n",
    "    A 2D mask tensor.\n",
    "    \"\"\"\n",
    "    # Get the length of the sequence\n",
    "    indices = torch.Tensor(range(seq_len+1))\n",
    "    repeats = indices[1:]\n",
    "    repeated_indices = torch.concat([indices, repeats])\n",
    "    seq_len = 2 * seq_len + 1\n",
    "\n",
    "    # Initialize an empty mask\n",
    "    mask = torch.zeros((seq_len, seq_len), dtype=torch.bool)\n",
    "    # Compare each token to all previous tokens - 1, for induction, skip the first token\n",
    "    for i in range(1, seq_len):\n",
    "        mask[i, 1:i] = (repeated_indices[i] == repeated_indices[:i-1])\n",
    "\n",
    "    return mask\n",
    "\n",
    "\n",
    "def induction_score_hook_inefficient(\n",
    "    pattern: torch.Tensor,\n",
    "    hook: HookPoint,\n",
    "    seq_len: int = 0,\n",
    "    threshold: float = 3\n",
    "):\n",
    "    '''\n",
    "    Calculates the induction score, and stores it in the [layer, head] position of the `induction_score_store` tensor.\n",
    "\n",
    "    pattern: Float[torch.Tensor, \"batch head_index dest_pos source_pos\"]\n",
    "    '''\n",
    "    induction_mask = create_induction_mask_rep_tokens(seq_len)\n",
    "    attention_pattern = pattern.mean(dim = 0).squeeze()\n",
    "\n",
    "    for head in range(attention_pattern.size()[0]):\n",
    "        curr_attention_pattern = attention_pattern[head].squeeze()\n",
    "        induction_activations = curr_attention_pattern[induction_mask]\n",
    "        \n",
    "        norm_attention_pattern = (curr_attention_pattern - curr_attention_pattern.mean()) / curr_attention_pattern.std()\n",
    "        norm_induction = (induction_activations - curr_attention_pattern.mean()) / curr_attention_pattern.std()\n",
    "        # Check if the mean induction token attn value is significantly larger than the mean activations\n",
    "        induction_score_store[hook.layer(), head] = norm_induction.mean() - (norm_attention_pattern.mean() + threshold * norm_attention_pattern.std())\n",
    "        rank_store[hook.layer(), head] = torch.linalg.matrix_rank(curr_attention_pattern)\n",
    "\n",
    "def induction_score_hook(\n",
    "    pattern: torch.Tensor,\n",
    "    hook: HookPoint,\n",
    "    seq_len: int = 0,\n",
    "    threshold: float = 3\n",
    "):\n",
    "    '''\n",
    "    Calculates the induction score, and stores it in the [layer, head] position of the `induction_score_store` tensor.\n",
    "\n",
    "    pattern: Float[torch.Tensor, \"batch head_index dest_pos source_pos\"]\n",
    "    '''\n",
    "    attention_pattern = pattern.mean(dim = 0).squeeze() # avg across batch\n",
    "    induction_mask = create_induction_mask_rep_tokens(seq_len).unsqueeze(0).expand_as(attention_pattern)\n",
    "\n",
    "    mean_induction_activations = attention_pattern[induction_mask].view(attention_pattern.size()[0], -1).mean(dim=1)\n",
    "    mean_activations = attention_pattern.mean(dim=[1,2])\n",
    "    std_activations = attention_pattern.std(dim=[1,2])\n",
    "    induction_scores = mean_induction_activations - (mean_activations + threshold * std_activations)\n",
    "    induction_score_store[hook.layer(), :] = induction_scores\n",
    "\n",
    "    ranks = torch.zeros(attention_pattern.size()[0])\n",
    "    for i in range(attention_pattern.size()[0]):\n",
    "        ranks[i] = torch.linalg.matrix_rank(attention_pattern[i])\n",
    "    rank_store[hook.layer(), :] = ranks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "induction_score_store = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
    "rank_store = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
    "\n",
    "seq_len = 50\n",
    "batch = 10\n",
    "rep_tokens_10 = generate_repeated_tokens(model, seq_len, batch)\n",
    "\n",
    "pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
    "\n",
    "induction_score_hook_function = functools.partial(\n",
    "    induction_score_hook_inefficient,\n",
    "    seq_len = seq_len,\n",
    "    threshold = 3\n",
    ")\n",
    "\n",
    "# Run with hooks (this is where we write to the `induction_score_store` tensor`)\n",
    "model.run_with_hooks(\n",
    "    rep_tokens_10, \n",
    "    return_type=None, # For efficiency, we don't need to calculate the logits\n",
    "    fwd_hooks=[(\n",
    "        pattern_hook_names_filter,\n",
    "        induction_score_hook_function\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Plot the induction scores for each head in each layer\n",
    "plotly_utils.imshow(\n",
    "    induction_score_store, \n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"}, \n",
    "    title=\"Induction Score by Head\", \n",
    "    text_auto=\".2f\",\n",
    "    width=900, height=400\n",
    ")\n",
    "\n",
    "plotly_utils.imshow(\n",
    "    rank_store, \n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"}, \n",
    "    title=\"Rank of average activation by Head\", \n",
    "    text_auto=\".2f\",\n",
    "    width=900, height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_pattern_hook(\n",
    "    pattern: torch.Tensor,\n",
    "    hook: HookPoint,\n",
    "    model = None,\n",
    "    tokens = None\n",
    "):\n",
    "    \"\"\"\n",
    "    pattern: Float[Tensor, \"batch head_index dest_pos source_pos\"]\n",
    "    \"\"\"\n",
    "    print(\"Layer: \", hook.layer())\n",
    "    display(\n",
    "        cv.attention.attention_patterns(\n",
    "            tokens=model.to_str_tokens(tokens[0]), \n",
    "            attention=pattern.mean(0)\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# on gpt2-small\n",
    "gpt2_small = HookedTransformer.from_pretrained(\"gpt2-small\")\n",
    "print(type(gpt2_small))\n",
    "print(f\"{gpt2_small.cfg=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "induction_score_store = torch.zeros((gpt2_small.cfg.n_layers, gpt2_small.cfg.n_heads), device=gpt2_small.cfg.device)\n",
    "rank_store = torch.zeros((gpt2_small.cfg.n_layers, gpt2_small.cfg.n_heads), device=gpt2_small.cfg.device)\n",
    "seq_len = 50\n",
    "batch = 10\n",
    "rep_tokens_10 = generate_repeated_tokens(gpt2_small, seq_len, batch)\n",
    "\n",
    "pattern_hook_names_filter = lambda name: name.endswith(\"pattern\")\n",
    "\n",
    "induction_score_hook_function = functools.partial(\n",
    "    induction_score_hook,\n",
    "    seq_len = seq_len,\n",
    "    threshold = 3\n",
    ")\n",
    "\n",
    "# Run with hooks (this is where we write to the `induction_score_store` tensor`)\n",
    "gpt2_small.run_with_hooks(\n",
    "    rep_tokens_10, \n",
    "    return_type=None, # For efficiency, we don't need to calculate the logits\n",
    "    fwd_hooks=[(\n",
    "        pattern_hook_names_filter,\n",
    "        induction_score_hook_function\n",
    "    )]\n",
    ")\n",
    "\n",
    "# Plot the induction scores for each head in each layer\n",
    "plotly_utils.imshow(\n",
    "    induction_score_store, \n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"}, \n",
    "    title=\"Induction Score by Head\", \n",
    "    text_auto=\".2f\",\n",
    "    width=900, height=400\n",
    ")\n",
    "\n",
    "plotly_utils.imshow(\n",
    "    rank_store, \n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"}, \n",
    "    title=\"Rank of average activation by Head\", \n",
    "    text_auto=\".2f\",\n",
    "    width=900, height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# visualize only layers 5, 6, and 7\n",
    "visualize_pattern_hook_function = functools.partial(\n",
    "    visualize_pattern_hook,\n",
    "    model = gpt2_small,\n",
    "    tokens = rep_tokens_10\n",
    ")\n",
    "pattern_hook_names_filter_of_interest = lambda name: name in [utils.get_act_name(\"pattern\", l) for l in (5, 6, 7)]\n",
    "\n",
    "# Run with hooks (this is where we write to the `induction_score_store` tensor`)\n",
    "gpt2_small.run_with_hooks(\n",
    "    rep_tokens_10, \n",
    "    return_type=None, # For efficiency, we don't need to calculate the logits\n",
    "    fwd_hooks=[(\n",
    "        pattern_hook_names_filter_of_interest,\n",
    "        visualize_pattern_hook_function\n",
    "    )]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logit Attribution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logit_attribution(\n",
    "    embed: torch.Tensor,\n",
    "    l1_results: torch.Tensor,\n",
    "    l2_results: torch.Tensor,\n",
    "    w_u: torch.Tensor,\n",
    "    tokens: torch.Tensor \n",
    "):\n",
    "    '''\n",
    "    Inputs:\n",
    "        embed: the embeddings of the tokens (i.e. token + position embeddings) Float[Tensor, \"seq d_model\"]\n",
    "        l1_results: the outputs of the attention heads at layer 1 (with head as one of the dimensions) Float[Tensor, \"seq nheads d_model\"]\n",
    "        l2_results: the outputs of the attention heads at layer 2 (with head as one of the dimensions) Float[Tensor, \"seq nheads d_model\"]\n",
    "        W_U: the unembedding matrix Float[Tensor, \"d_model d_vocab\"]\n",
    "        tokens: the token ids of the sequence Int[Tensor, \"seq\"]\n",
    "\n",
    "    Returns:\n",
    "        Tensor of shape (seq_len-1, n_components) -> Float[Tensor, \"seq-1 n_components\"]\n",
    "        represents the concatenation (along dim=-1) of logit attributions from:\n",
    "            the direct path (seq-1,1)\n",
    "            layer 0 logits (seq-1, n_heads)\n",
    "            layer 1 logits (seq-1, n_heads)\n",
    "        so n_components = 1 + 2*n_heads\n",
    "    '''\n",
    "    w_u_correct_tokens = w_u[:, tokens[1:]]\n",
    "\n",
    "    # direct path\n",
    "    direct_path_logits = torch.diagonal(embed @ w_u_correct_tokens).unsqueeze(dim=1)\n",
    "    \n",
    "    # layer 1 path\n",
    "    layer_1_logits = torch.diagonal(\n",
    "        torch.einsum(\"imh,mo->ioh\", l1_results.permute(0, 2, 1), w_u_correct_tokens),\n",
    "        offset = 0, dim1 = 0, dim2 = 1\n",
    "    ).T\n",
    "\n",
    "    # layer 2 path\n",
    "    layer_2_logits = torch.diagonal(\n",
    "        torch.einsum(\"imh,mo->ioh\", l2_results.permute(0, 2, 1), w_u_correct_tokens),\n",
    "        offset = 0, dim1 = 0, dim2 = 1\n",
    "    ).T\n",
    "\n",
    "    # # inefficient (but more readable?) way to do it, by indexing into the logits to get the right token after unembedding!\n",
    "    # direct_path_logits = (embed @ W_U)[torch.arange(tokens.size()[0] - 1), tokens[1:]].unsqueeze(dim=1)\n",
    "\n",
    "    # # layer 1 path\n",
    "    # layer_1_logits = torch.einsum(\"smh,mv->svh\", l1_results.permute(0, 2, 1), W_U)[\n",
    "    #     torch.arange(tokens.size()[0] - 1), tokens[1:]\n",
    "    # ]\n",
    "    # layer_1_logits = layer_1_logits.view(tokens.size()[0] - 1, -1)\n",
    "\n",
    "    # # layer 2 path\n",
    "    # layer_2_logits = torch.einsum(\"smh,mv->svh\", l2_results.permute(0, 2, 1), W_U)[\n",
    "    #     torch.arange(tokens.size()[0] - 1), tokens[1:]\n",
    "    # ]\n",
    "    # layer_2_logits = layer_2_logits.view(tokens.size()[0] - 1, -1)\n",
    "    \n",
    "    print(direct_path_logits.size(), layer_1_logits.size(), layer_2_logits.size())\n",
    "\n",
    "    return torch.concat([direct_path_logits, layer_1_logits, layer_2_logits], dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_text = \"We think that powerful, significantly superhuman machine intelligence is more likely than not to be created this century. If current machine learning techniques were scaled up to this level, we think they would by default produce systems that are deceptive or manipulative, and that no solid plans are known for how to avoid this.\"\n",
    "logits, cache = model.run_with_cache(input_text, remove_batch_dim=True)\n",
    "str_tokens = model.to_str_tokens(input_text)\n",
    "tokens = model.to_tokens(input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    embed = cache[\"embed\"]\n",
    "    l1_results = cache[\"result\", 0]\n",
    "    l2_results = cache[\"result\", 1]\n",
    "    logit_attr = logit_attribution(embed, l1_results, l2_results, model.W_U, tokens[0])\n",
    "    # Uses fancy indexing to get a len(tokens[0])-1 length tensor, where the kth entry is the predicted logit for the correct k+1th token\n",
    "    correct_token_logits = logits[0, torch.arange(len(tokens[0]) - 1), tokens[0, 1:]]\n",
    "    torch.testing.assert_close(logit_attr.sum(1), correct_token_logits, atol=1e-3, rtol=0)\n",
    "    print(\"Tests passed!\")\n",
    "\n",
    "    plotly_utils.plot_logit_attribution(model, logit_attr, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run it through gpt-2 (layers 5 and 6) -> Actually the computations are for attention only, the non-linearities in GPT-2 make\n",
    "# it tricky, or rather we would need to re-write the logit attribution for gpt-2!\n",
    "# gpt2_logits, gpt2_cache = gpt2_small.run_with_cache(input_text, remove_batch_dim=True)\n",
    "# gpt2_str_tokens = gpt2_small.to_str_tokens(input_text)\n",
    "# gpt2_tokens = gpt2_small.to_tokens(input_text)\n",
    "\n",
    "# with torch.inference_mode():\n",
    "#     gpt2_embed = cache[\"embed\"]\n",
    "#     gpt2_l5_results = cache[\"result\", 5]\n",
    "#     gpt2_l6_results = cache[\"result\", 6]\n",
    "#     gpt2_logit_attr = logit_attribution(gpt2_embed, gpt2_l5_results, gpt2_l6_results, gpt2_small.W_U, gpt2_tokens[0])\n",
    "#     # Uses fancy indexing to get a len(tokens[0])-1 length tensor, where the kth entry is the predicted logit for the correct k+1th token\n",
    "#     gpt2_correct_token_logits = gpt2_logits[0, torch.arange(len(tokens[0]) - 1), tokens[0, 1:]]\n",
    "#     torch.testing.assert_close(gpt2_logit_attr.sum(1), gpt2_correct_token_logits, atol=1e-3, rtol=0)\n",
    "#     print(\"Tests passed!\")\n",
    "\n",
    "#     plotly_utils.plot_logit_attribution(gpt2_small, gpt2_logit_attr, tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 50\n",
    "rep_tokens = generate_repeated_tokens(model, seq_len, batch=1)\n",
    "rep_logits, rep_cache = model.run_with_cache(rep_tokens, remove_batch_dim=True)\n",
    "\n",
    "with torch.inference_mode():\n",
    "    embed = rep_cache[\"embed\"]\n",
    "    l1_results = rep_cache[\"result\", 0]\n",
    "    l2_results = rep_cache[\"result\", 1]\n",
    "    first_half_tokens = rep_tokens[0, :seq_len+1]\n",
    "    second_half_tokens = rep_tokens[0, seq_len:]\n",
    "    first_half_logit_attr = logit_attribution(embed[:seq_len+1], l1_results[:seq_len+1], l2_results[:seq_len+1], model.W_U, first_half_tokens)\n",
    "    second_half_logit_attr = logit_attribution(embed[seq_len:], l1_results[seq_len:], l2_results[seq_len:], model.W_U, second_half_tokens)\n",
    "\n",
    "assert first_half_logit_attr.shape == (seq_len, 2*model.cfg.n_heads + 1)\n",
    "assert second_half_logit_attr.shape == (seq_len, 2*model.cfg.n_heads + 1)\n",
    "\n",
    "plotly_utils.plot_logit_attribution(model, first_half_logit_attr, first_half_tokens, \"Logit attribution (first half of repeated sequence)\")\n",
    "plotly_utils.plot_logit_attribution(model, second_half_logit_attr, second_half_tokens, \"Logit attribution (second half of repeated sequence)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ablations\n",
    "def head_ablation_hook(\n",
    "    v: torch.Tensor,\n",
    "    hook: HookPoint,\n",
    "    head_index_to_ablate: int\n",
    "):\n",
    "    \"\"\"\n",
    "    v: Float[Tensor, \"batch seq n_heads d_head\"]\n",
    "    out -> Float[Tensor, \"batch seq n_heads d_head\"]\n",
    "    \"\"\"\n",
    "    v[:, :, head_index_to_ablate, :] = 0.0\n",
    "    return v\n",
    "\n",
    "def cross_entropy_loss(logits, tokens):\n",
    "    '''\n",
    "    Computes the mean cross entropy between logits (the model's prediction) and tokens (the true values).\n",
    "\n",
    "    (optional, you can just use return_type=\"loss\" instead.)\n",
    "    '''\n",
    "    log_probs = torch.log_softmax(logits, dim=-1)\n",
    "    pred_log_probs = torch.gather(log_probs[:, :-1], -1, tokens[:, 1:, None])[..., 0]\n",
    "    return -pred_log_probs.mean()\n",
    "\n",
    "\n",
    "def get_ablation_scores(\n",
    "    model: HookedTransformer, \n",
    "    tokens: torch.Tensor,\n",
    "):\n",
    "    '''\n",
    "    Returns a tensor of shape (n_layers, n_heads) containing the increase in cross entropy loss from ablating the output of each head.\n",
    "\n",
    "    tokens: Int[Tensor, \"batch seq\"]\n",
    "    out: -> Float[Tensor, \"n_layers n_heads\"]\n",
    "    '''\n",
    "    # Initialize an object to store the ablation scores\n",
    "    ablation_scores = torch.zeros((model.cfg.n_layers, model.cfg.n_heads), device=model.cfg.device)\n",
    "\n",
    "    # Calculating loss without any ablation, to act as a baseline\n",
    "    model.reset_hooks()\n",
    "    logits = model(tokens, return_type=\"logits\")\n",
    "    loss_no_ablation = cross_entropy_loss(logits, tokens)\n",
    "\n",
    "    for layer in tqdm(range(model.cfg.n_layers)):\n",
    "        for head in range(model.cfg.n_heads):\n",
    "            # Use functools.partial to create a temporary hook function with the head number fixed\n",
    "            temp_hook_fn = functools.partial(head_ablation_hook, head_index_to_ablate=head)\n",
    "            # Run the model with the ablation hook\n",
    "            ablated_logits = model.run_with_hooks(tokens, fwd_hooks=[\n",
    "                (utils.get_act_name(\"v\", layer), temp_hook_fn)\n",
    "            ])\n",
    "            # Calculate the logit difference\n",
    "            loss = cross_entropy_loss(ablated_logits, tokens)\n",
    "            # Store the result, subtracting the clean loss so that a value of zero means no change in loss\n",
    "            ablation_scores[layer, head] = loss - loss_no_ablation\n",
    "\n",
    "    return ablation_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ablation_scores = get_ablation_scores(model, rep_tokens)\n",
    "plotly_utils.imshow(\n",
    "    ablation_scores, \n",
    "    labels={\"x\": \"Head\", \"y\": \"Layer\"}, \n",
    "    title=\"Ablation score by Head\", \n",
    "    text_auto=\".2f\",\n",
    "    width=900, height=400\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution from Callum McDougall's files\n",
    "# from callummcdougall_chapters.chapter1_transformers.exercises.part2_intro_to_mech_interp import tests\n",
    "# tests.test_get_ablation_scores(ablation_scores, model, rep_tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bonus: got bored to implement, \n",
    "# but performance should improve when you ablate everything except the previous token head & two induction heads\n",
    "# as they seem to influence the output the most!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "transformers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
